{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3657b3a9-734c-4026-a861-ccfb709ac9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e3b495-aa6c-401c-9ebb-9b0af3948f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Required Libraries and Modules\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import importlib as lib\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Data Manipulation and Configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# PyTorch Utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "# Image Processing Libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Compose, Lambda\n",
    "\n",
    "# Utility Functions\n",
    "from utils import mnist_dataset\n",
    "from utils import dataset_getter as dat\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Logic Layer Implementation\n",
    "from difflogic import LogicLayer, GroupSum, PackBitsTensor\n",
    "import difflogic_cuda\n",
    "\n",
    "# Configuration Management with Hydra\n",
    "from hydra import initialize, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af6a84f-c3f4-4fe6-afb7-da89918366bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Display Available GPU Information\n",
    "print(f\"Detected GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f635a35-7354-449f-9523-dca306ad331f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8d4caf-68c9-40cf-8753-321d7fa11678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/woodard/mkunzlermaldaner/EXPLOGIC/explogic_env/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Train Dataset Size: 54210\n",
      "Balanced Test Dataset Size: 8920\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset and Preprocessing Settings\n",
    "crop = (20,20)  # Cropping configuration: None removes no border, specify (x,x) for cropping dimensions\n",
    "\n",
    "# Dataset Selection\n",
    "# Options: \"mnist\" or \"fashion_mnist\"\n",
    "dataset_name = \"mnist\"\n",
    "\n",
    "# Bits Per Pixel (Fixed at 1 for binary data in ExpLogic)\n",
    "bpp = 1\n",
    "\n",
    "# Ensuring Reproducibility Across Runs\n",
    "# Seeds for Random Generators in PyTorch, NumPy, and Python\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# DataLoader Parameters\n",
    "batch_size = 512\n",
    "train_loader, test_loader, input_dim, out_dim = dat.get_dataset(\n",
    "    dataset_name, batch_size=batch_size, data_dir=\"./data\", bpp=bpp, crop=crop\n",
    ")\n",
    "\n",
    "# Analyze and Balance Class Distribution in Dataset\n",
    "train_targets = train_loader.dataset.targets\n",
    "test_targets = test_loader.dataset.targets\n",
    "\n",
    "# Class-Wise Sample Count\n",
    "train_class_counts = [torch.sum(train_targets == i).item() for i in range(10)]\n",
    "test_class_counts = [torch.sum(test_targets == i).item() for i in range(10)]\n",
    "\n",
    "# Determine Minimum Samples Per Class for Balanced Dataset\n",
    "min_samples_train = min(train_class_counts)\n",
    "min_samples_test = min(test_class_counts)\n",
    "\n",
    "# Define Function to Trim Datasets for Balance\n",
    "def balance_dataset(dataset, targets, min_samples):\n",
    "    indices = []\n",
    "    for class_label in range(10):\n",
    "        class_indices = (targets == class_label).nonzero(as_tuple=True)[0]\n",
    "        indices.extend(class_indices[:min_samples])\n",
    "\n",
    "    # Shuffle Indices to Randomize the Dataset\n",
    "    indices = torch.tensor(indices)\n",
    "    shuffled_indices = indices[torch.randperm(indices.size(0))]\n",
    "    \n",
    "    return Subset(dataset, shuffled_indices)\n",
    "\n",
    "# Apply Balancing to Train and Test Datasets\n",
    "balanced_train_dataset = balance_dataset(train_loader.dataset, train_targets, min_samples_train)\n",
    "balanced_test_dataset = balance_dataset(test_loader.dataset, test_targets, min_samples_test)\n",
    "\n",
    "# Re-Initialize DataLoaders with Balanced Datasets\n",
    "train_loader_balanced = DataLoader(\n",
    "    balanced_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True\n",
    ")\n",
    "test_loader_balanced = DataLoader(\n",
    "    balanced_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True\n",
    ")\n",
    "\n",
    "# Verify New Dataset Sizes\n",
    "print(f\"Balanced Train Dataset Size: {len(train_loader_balanced.dataset)}\")\n",
    "print(f\"Balanced Test Dataset Size: {len(test_loader_balanced.dataset)}\")\n",
    "\n",
    "# Update Loaders and Datasets for Subsequent Use\n",
    "train_loader = train_loader_balanced\n",
    "test_loader = test_loader_balanced\n",
    "\n",
    "# Visualize a Random Image from the Balanced Training Dataset\n",
    "data_index = random.randint(0, len(train_loader.dataset) - 1)\n",
    "random_image, _ = train_loader.dataset[data_index]\n",
    "\n",
    "# Process Image for Display\n",
    "image_shape = (20, 20) if crop else (28, 28)\n",
    "processed_image = np.array([\n",
    "    np.sum([random_image[(i * bpp) + j] * (2 ** (bpp - j + 1)) for j in range(bpp)])\n",
    "    for i in range(image_shape[0] * image_shape[0])\n",
    "]).reshape(image_shape)\n",
    "\n",
    "# Plot the Processed Image\n",
    "plt.figure()\n",
    "plt.imshow(processed_image, cmap=\"gray\")\n",
    "plt.title(\"Sample Image from Balanced Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c940db-45b8-485c-90d9-f5c48474e11a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Model Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485ca94-7a5f-4f14-89c5-4767f8665e79",
   "metadata": {
    "tags": []
   },
   "source": [
    "Converts csv into yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a7f3c1-3f23-4448-8155-8fc4a449e59f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define first input and the name of the file to be saved\n",
    "if dataset_name == \"mnist\":\n",
    "    first_in_dim = 400*bpp\n",
    "    filename = \"config/mnist_config_20x20.yaml\"\n",
    "    df = pd.read_csv(\"config/mnist_hyperparameters.csv\")\n",
    "elif dataset_name == \"fashion_mnist\":\n",
    "    first_in_dim = 784*bpp\n",
    "    filename = \"config/fashion_mnist_config_28x28.yaml\"\n",
    "    df = pd.read_csv(\"config/fashion_mnist_hyperparameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff39cc4e-6558-48d6-a671-1a4583301a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file 'config/mnist_config_20x20.yaml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# convert the DataFrame to a list of dictionaries\n",
    "models = df.to_dict(orient=\"records\")\n",
    "\n",
    "# create the YAML structure\n",
    "yaml_structure = {\"models\": {}}\n",
    "\n",
    "# rounds the number to the nearest multiple of the output size\n",
    "def round_to_nearest_multiple(value, multiple):\n",
    "    return multiple * round(value / multiple)\n",
    "\n",
    "# populate the YAML structure with models\n",
    "for i, model in enumerate(models, start=1):\n",
    "    # zero-padding model names to 3 digits \n",
    "    model_name = f\"model_{str(i).zfill(3)}\"\n",
    "    layers_config = {}\n",
    "    \n",
    "    for layer in range(1, model[\"H\"] + 1):\n",
    "        # zero-padding the layer names to 3 digits\n",
    "        layer_name = f\"LogicLayer{str(layer).zfill(3)}\"\n",
    "        \n",
    "        # adjusts in_dim to the nearest multiple of 10\n",
    "        in_dim = first_in_dim if layer == 1 else round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        # adjusts out_dim to the nearest multiple of 10\n",
    "        out_dim = round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        layers_config[layer_name] = {\n",
    "            \"in_dim\": in_dim,\n",
    "            \"out_dim\": out_dim,\n",
    "            \"device\": \"cuda\",\n",
    "            \"implementation\": \"cuda\",\n",
    "            \"connections\": \"random\",\n",
    "            \"grad_factor\": 2, # we can try different grad_factor values as well\n",
    "        }\n",
    "    \n",
    "    yaml_structure[\"models\"][model_name] = {\n",
    "        \"input_dim\": first_in_dim, \n",
    "        \"output_size\": 10, # for MNIST classification\n",
    "        \"tau\": model[\"tau\"],\n",
    "        \"learning_rate\": model[\"lr\"],\n",
    "        \"layers_config\": layers_config,\n",
    "    }\n",
    "\n",
    "# saves to a YAML file\n",
    "with open(f'{filename}', \"w\") as file:\n",
    "    yaml.dump(yaml_structure, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file '{filename}' generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb817b1-81d9-40a0-87af-c9ad5bdf7fed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Model Function Declarations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f4c875-b543-414c-8057-1eef2ed20b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiffLogic(nn.Module):\n",
    "    def __init__(self, layers_config, output_size, tau=30):\n",
    "        \"\"\"\n",
    "        Initializes the DiffLogic model with the specified layer configurations, output size, and temperature parameter.\n",
    "\n",
    "        Args:\n",
    "            layers_config (dict): Configuration for each logic layer, including dimensions, device, implementation, connections, and grad factor.\n",
    "            output_size (int): The number of output groups (classes in a classification problem).\n",
    "            tau (int): Temperature parameter for the GroupSum operation.\n",
    "        \"\"\"\n",
    "        super(DiffLogic, self).__init__()\n",
    "        \n",
    "        # stores the logic layers\n",
    "        layers = []\n",
    "        for layer_name, config in layers_config.items():\n",
    "            layer = LogicLayer(\n",
    "                in_dim=config['in_dim'],\n",
    "                out_dim=config['out_dim'],\n",
    "                device=config['device'],\n",
    "                implementation=config['implementation'],\n",
    "                connections=config['connections'],\n",
    "                grad_factor=config['grad_factor']       \n",
    "            )\n",
    "            layers.append(layer)\n",
    "            print(layer)\n",
    "        \n",
    "        self.logic_layers = nn.Sequential(*layers)\n",
    "        self.group = GroupSum(k=output_size, tau=tau)\n",
    "        self.log_text = \"\"  # initializes logging string\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DiffLogic model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through the logic layers and grouping operation.\n",
    "        \"\"\"\n",
    "        # moves tensor to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to('cuda')          \n",
    "        #x = torch.flatten(x)\n",
    "        logits = self.logic_layers(x)\n",
    "        group = self.group(logits)\n",
    "        return group\n",
    "    \n",
    "    def save(self, file_path, model_name='model', model_cfg=None):\n",
    "        \"\"\"\n",
    "        Saves the model's state dictionary plus all relevant architecture info\n",
    "        to the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path where the model will be saved.\n",
    "            model_name (str): Name of the saved model.\n",
    "            model_cfg (dict): The full dictionary that contains layer config,\n",
    "                              output_size, tau, learning rate, etc.\n",
    "        \"\"\"\n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'connections': [\n",
    "                layer.indices \n",
    "                for layer in self.logic_layers \n",
    "                if isinstance(layer, LogicLayer)\n",
    "            ],\n",
    "            # Store the entire config so you can reconstruct everything\n",
    "            'model_config': {\n",
    "                'layers_config': model_cfg['layers_config'],\n",
    "                'output_size': model_cfg['output_size'],\n",
    "                'tau': model_cfg['tau'],\n",
    "                'learning_rate': model_cfg.get('learning_rate', None),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Now dump that all out to disk\n",
    "        torch.save(checkpoint_data, os.path.join(file_path, f\"{model_name}.pth\"))\n",
    "        self.log_text += f\"Model saved to: {file_path}\\n\"\n",
    "\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the model's state dictionary from the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path from which the model will be loaded.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # assigns connections to each LogicLayer\n",
    "        for idx, layer in enumerate(self.logic_layers):\n",
    "            if isinstance(layer, LogicLayer):\n",
    "                layer.indices = checkpoint['connections'][idx]\n",
    "\n",
    "        self.eval()\n",
    "        self.log_text += f\"Model loaded from: {file_path}\\n\"\n",
    "        \n",
    "    def get_accuracy(self, data_loader):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the model against a data loader\n",
    "\n",
    "        Args:\n",
    "            data_loader: a DataLoader object, e.g. train_loader or test_loader\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # ensures that model is in evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Running Inference\"):\n",
    "                batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "                # forward pass to get predictions\n",
    "                outputs = self(batch_inputs.float())\n",
    "\n",
    "                # gets the predicted class (index of the maximum logit)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # counting correct predictions\n",
    "                total += batch_outputs.size(0)  # total number of samples in the batch\n",
    "                correct += (predicted == batch_outputs).sum().item()  # counting correct predictions\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def get_log(self):\n",
    "        \"\"\"\n",
    "        Retrieves the log text and clears the log after retrieval.\n",
    "\n",
    "        Returns:\n",
    "            str: The log text.\n",
    "        \"\"\"\n",
    "        log_copy = self.log_text\n",
    "        self.log_text = \"\"  # Clear the log after returning\n",
    "        return log_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e1677a-28ff-41fd-9892-987011fa85da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopper to stop training if the performance doesn't improve after a certain number of epochs.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait for an improvement.\n",
    "            min_delta (float): Minimum change to consider an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def should_stop(self, current_loss):\n",
    "        \"\"\"\n",
    "        Check if training should stop based on the current loss.\n",
    "\n",
    "        Args:\n",
    "            current_loss (float): The current loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            return False\n",
    "        elif current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopper Triggered: \", self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761ed30-7e5c-4b77-8ad0-db1c0e0a3db1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21ebed7-b243-49fb-9731-ed6dc454480e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model model_001\n",
      "LogicLayer(400, 2500, train)\n",
      "LogicLayer(2500, 2500, train)\n",
      "model DiffLogic(\n",
      "  (logic_layers): Sequential(\n",
      "    (0): LogicLayer(400, 2500, train)\n",
      "    (1): LogicLayer(2500, 2500, train)\n",
      "  )\n",
      "  (group): GroupSum(k=10, tau=10)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 105/105 [00:58<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.8179197958773952\n",
      "All models processed.\n"
     ]
    }
   ],
   "source": [
    "# initialize Hydra with the config path and job name\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"Visualizer\"):\n",
    "    cfg = compose(config_name=f\"{dataset_name}_config_{image_shape[0]}x{image_shape[0]}\")\n",
    "\n",
    "# training loop for all models\n",
    "all_models_dict = {}\n",
    "num_epochs = 1 # just for debugging\n",
    "file_path = f'trained_models/{dataset_name}_trained_models' # where to save your trained models\n",
    "\n",
    "# loops through all model configs and trains each of them\n",
    "for model_name, model_cfg in cfg.models.items():\n",
    "    print(f'training model {model_name}')\n",
    "\n",
    "    # tracking dictionary\n",
    "    all_models_dict[model_name] = {\n",
    "        'losses': [],\n",
    "    }\n",
    "\n",
    "    # initializes DiffLogic model and moves to CUDA if available\n",
    "    model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                      output_size=model_cfg['output_size'], \n",
    "                      tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # wraps the model with DataParallel to distribute workload across GPUs \n",
    "    #model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    # optimizer and loss criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # early stopping\n",
    "    early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "    print('model', model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        epoch_loss = 0  # to track loss for an epoch            \n",
    "\n",
    "        for batch_inputs, batch_outputs in loop:\n",
    "            # move data to the appropriate device\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            batch_inputs, batch_outputs = batch_inputs.to(device).double(), batch_outputs.to(device).long()\n",
    "\n",
    "            # forward pass through the model\n",
    "            predictions = model(batch_inputs)  \n",
    "                        \n",
    "            loss = criterion(predictions, batch_outputs)\n",
    "            \n",
    "            # zero gradients, backpropagates, and updates model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # accumulating the loss for the epoch\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # caclulating the average loss for the epoch\n",
    "        epoch_loss /= len(train_loader)\n",
    "        all_models_dict[model_name]['losses'].append(epoch_loss)\n",
    "        print(f'Epoch {epoch+1} Loss: {epoch_loss}')\n",
    "\n",
    "        # checks for early stopping\n",
    "        if early_stopper.should_stop(epoch_loss):\n",
    "            print(f\"Early stopping triggered for {model_name} at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "    # saving trained model's state\n",
    "    model.save(file_path, model_name, model_cfg=model_cfg)\n",
    "\n",
    "print(\"All models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1583a9-f0c6-4381-8231-ea2d55668665",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Model Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cfb5acb-96dc-4b53-9560-9dea237c164d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicLayer(400, 2500, train)\n",
      "LogicLayer(2500, 2500, train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiffLogic(\n",
       "  (logic_layers): Sequential(\n",
       "    (0): LogicLayer(400, 2500, eval)\n",
       "    (1): LogicLayer(2500, 2500, eval)\n",
       "  )\n",
       "  (group): GroupSum(k=10, tau=10)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'model_001'\n",
    "file_path = f'trained_models/{dataset_name}_trained_models/{model_name}.pth' # where to save your trained models\n",
    "checkpoint = torch.load(file_path, map_location=\"cuda\")  # or \"cuda\"\n",
    "loaded_config = checkpoint['model_config']\n",
    "\n",
    "\n",
    "# Now reconstruct the model using the saved config\n",
    "model_reconstructed = DiffLogic(\n",
    "    layers_config=loaded_config['layers_config'],\n",
    "    output_size=loaded_config['output_size'],\n",
    "    tau=loaded_config['tau']\n",
    ")\n",
    "\n",
    "# Load the state_dict\n",
    "model_reconstructed.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# If needed, also restore the connections\n",
    "for idx, layer in enumerate(model_reconstructed.logic_layers):\n",
    "    if isinstance(layer, LogicLayer):\n",
    "        layer.indices = checkpoint['connections'][idx]\n",
    "\n",
    "model_reconstructed.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd17cfb4-a301-4d21-98b4-b8fdb346a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 17/17 [00:12<00:00,  1.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7548253676470589"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reconstructed.get_accuracy(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15edd58-29d0-46e6-9561-5a47b872a047",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Get All Connections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7221f442-5d88-4d86-a091-ae0ff54c5620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neuron_idx': 0, 'gate': 'and', 'inputs': (241, 12)},\n",
       " {'neuron_idx': 1, 'gate': 'not_b', 'inputs': (309, 388)},\n",
       " {'neuron_idx': 2, 'gate': 'xor', 'inputs': (252, 387)},\n",
       " {'neuron_idx': 3, 'gate': 'xor', 'inputs': (253, 57)},\n",
       " {'neuron_idx': 4, 'gate': 'not_xor', 'inputs': (263, 190)}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_connections(model):\n",
    "    \n",
    "    # map gate names to indices\n",
    "    ALL_OPERATIONS = [\n",
    "        \"zero\", \"and\", \"not_implies\", \"a\", \"not_implied_by\", \"b\", \"xor\", \"or\", \n",
    "        \"not_or\", \"not_xor\", \"not_b\", \"implied_by\", \"not_a\", \"implies\", \"not_and\", \"one\"\n",
    "    ]\n",
    "\n",
    "    gates_per_layer = []\n",
    "    \n",
    "    for layer_idx, layer in enumerate(model.logic_layers):\n",
    "        layer_connections = []\n",
    "        \n",
    "        # for each neuron in the layer\n",
    "        for neuron_idx in range(layer.weights.size(0)):\n",
    "            # get the learned gate by taking the argmax of the weights for the neuron\n",
    "            gate_op_idx = layer.weights[neuron_idx].argmax().item()\n",
    "            learned_gate = ALL_OPERATIONS[gate_op_idx]\n",
    "\n",
    "            # get the input connections (indices) for the gate\n",
    "            input_neuron_a = layer.indices[0][neuron_idx].item()\n",
    "            input_neuron_b = layer.indices[1][neuron_idx].item()\n",
    "\n",
    "            # store the gate and connections\n",
    "            layer_connections.append({\n",
    "                'neuron_idx': neuron_idx,\n",
    "                'gate': learned_gate,\n",
    "                'inputs': (input_neuron_a, input_neuron_b),\n",
    "            })\n",
    "        \n",
    "        gates_per_layer.append(layer_connections)\n",
    "    \n",
    "    return gates_per_layer\n",
    "\n",
    "# retrieve all connections for the model\n",
    "connections = get_all_connections(model)\n",
    "connections[0][:5] # example displaying the first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8329f55-0480-4928-8b8c-c0dc078072cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Convert into Json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9549629a-0208-4665-a8d1-582c8c0b24fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_connections_to_json(connections):\n",
    "    layer_specs = []\n",
    "    for layer_idx, layer_conns in enumerate(connections):\n",
    "        layer_data = []\n",
    "        for conn in layer_conns:\n",
    "            # Convert 'inputs' tuple to a list so JSON will handle it\n",
    "            inputs_list = list(conn[\"inputs\"])  # e.g. (309, 103) -> [309, 103]\n",
    "\n",
    "            # Build each neuron connection dict\n",
    "            layer_data.append({\n",
    "                \"neuron_idx\": conn[\"neuron_idx\"],\n",
    "                \"gate\": conn[\"gate\"],\n",
    "                \"inputs\": inputs_list\n",
    "            })\n",
    "\n",
    "        # One entry per layer\n",
    "        layer_specs.append({\n",
    "            \"layer_idx\": layer_idx,\n",
    "            \"connections\": layer_data\n",
    "        })\n",
    "\n",
    "    # Pretty-print JSON string\n",
    "    return json.dumps(layer_specs, indent=2)\n",
    "\n",
    "connections_json = transform_connections_to_json(connections)\n",
    "with open(\"connections.json\", \"w\") as f:\n",
    "    f.write(connections_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83376d-7ae1-43cd-ab6d-6abe5c540005",
   "metadata": {},
   "source": [
    "#### **Extracting Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c1aaae8-bd0f-4968-adf6-50f3710acf00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'model_001'\n",
    "file_path = f'trained_models/{dataset_name}_trained_models/{model_name}.pth' # where to save your trained models\n",
    "checkpoint = torch.load(file_path, map_location=\"cuda\")  # or \"cuda\"\n",
    "loaded_config = checkpoint['model_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "684e1ae4-8f99-4abc-80be-e18d845d263c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_state_dict': OrderedDict([('logic_layers.0.weights',\n",
       "               tensor([[-0.0672,  1.9220, -0.9968,  ...,  0.9607, -1.1374,  1.8655],\n",
       "                       [-1.5337, -0.4288,  0.9440,  ..., -2.2876, -2.4655, -2.5941],\n",
       "                       [-0.0421,  0.3794, -0.3490,  ...,  0.6202, -0.0378,  0.5037],\n",
       "                       ...,\n",
       "                       [-0.2570, -0.9002,  0.1156,  ..., -1.8007, -0.1222, -1.4934],\n",
       "                       [-1.4102, -0.9311,  1.7573,  ..., -1.8845, -2.0175, -0.1830],\n",
       "                       [ 1.2210,  0.2777,  0.3703,  ...,  0.9924, -0.8301, -2.5824]],\n",
       "                      device='cuda:0')),\n",
       "              ('logic_layers.1.weights',\n",
       "               tensor([[-0.2500,  0.8593, -1.2279,  ..., -1.4946, -1.0206,  0.0125],\n",
       "                       [-1.1058,  0.9717, -0.9786,  ...,  1.8457, -2.3426, -1.2387],\n",
       "                       [-0.7938, -0.1460, -1.4591,  ..., -0.6128,  0.7427,  0.5009],\n",
       "                       ...,\n",
       "                       [-0.1099, -0.2998, -0.7317,  ..., -1.1587,  2.0011, -1.1070],\n",
       "                       [ 0.9442,  1.2687,  0.6751,  ...,  1.5658,  0.2423,  0.7008],\n",
       "                       [-1.2101,  0.1553, -0.6167,  ...,  0.4056, -0.2175, -0.2946]],\n",
       "                      device='cuda:0'))]),\n",
       " 'connections': [(tensor([241, 309, 252,  ..., 162, 225,  49], device='cuda:0'),\n",
       "   tensor([ 12, 388, 387,  ..., 381,  26, 203], device='cuda:0')),\n",
       "  (tensor([1983, 1790,  750,  ...,  759,   75, 1733], device='cuda:0'),\n",
       "   tensor([1293,  521, 2334,  ..., 2137, 1060,  547], device='cuda:0'))],\n",
       " 'model_config': {'layers_config': {'LogicLayer001': {'connections': 'random', 'device': 'cuda', 'grad_factor': 2, 'implementation': 'cuda', 'in_dim': 400, 'out_dim': 2500}, 'LogicLayer002': {'connections': 'random', 'device': 'cuda', 'grad_factor': 2, 'implementation': 'cuda', 'in_dim': 2500, 'out_dim': 2500}},\n",
       "  'output_size': 10,\n",
       "  'tau': 10,\n",
       "  'learning_rate': 0.01}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8851129-f342-4154-bfdc-60da4e297e64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: logic_layers.0.weights, Shape: torch.Size([2500, 16]), Device: cuda:0\n",
      "Layer: logic_layers.1.weights, Shape: torch.Size([2500, 16]), Device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logic_layers.0.weights': tensor([[-0.0672,  1.9220, -0.9968,  ...,  0.9607, -1.1374,  1.8655],\n",
       "         [-1.5337, -0.4288,  0.9440,  ..., -2.2876, -2.4655, -2.5941],\n",
       "         [-0.0421,  0.3794, -0.3490,  ...,  0.6202, -0.0378,  0.5037],\n",
       "         ...,\n",
       "         [-0.2570, -0.9002,  0.1156,  ..., -1.8007, -0.1222, -1.4934],\n",
       "         [-1.4102, -0.9311,  1.7573,  ..., -1.8845, -2.0175, -0.1830],\n",
       "         [ 1.2210,  0.2777,  0.3703,  ...,  0.9924, -0.8301, -2.5824]],\n",
       "        device='cuda:0'),\n",
       " 'logic_layers.1.weights': tensor([[-0.2500,  0.8593, -1.2279,  ..., -1.4946, -1.0206,  0.0125],\n",
       "         [-1.1058,  0.9717, -0.9786,  ...,  1.8457, -2.3426, -1.2387],\n",
       "         [-0.7938, -0.1460, -1.4591,  ..., -0.6128,  0.7427,  0.5009],\n",
       "         ...,\n",
       "         [-0.1099, -0.2998, -0.7317,  ..., -1.1587,  2.0011, -1.1070],\n",
       "         [ 0.9442,  1.2687,  0.6751,  ...,  1.5658,  0.2423,  0.7008],\n",
       "         [-1.2101,  0.1553, -0.6167,  ...,  0.4056, -0.2175, -0.2946]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_model_parameters(state_dict):\n",
    "    \"\"\"\n",
    "    Extracts and prints the weights from a given model state dictionary.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): The state dictionary containing model parameters.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are layer names and values are their corresponding tensors.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "\n",
    "    for name, tensor in state_dict.items():\n",
    "        parameters[name] = tensor\n",
    "        print(f\"Layer: {name}, Shape: {tensor.shape}, Device: {tensor.device}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "extract_model_parameters(checkpoint['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXPLOGIC",
   "language": "python",
   "name": "explogic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
